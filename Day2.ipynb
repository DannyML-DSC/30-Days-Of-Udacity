{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyObXqGyUp2ZscTITIUH5YPJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DannyML-DSC/30-Days-Of-Udacity/blob/master/Day2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuiHklujzLjr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "d62d6c48-63be-4e2f-f72a-216da65b9fab"
      },
      "source": [
        "#authenticatiopn script in gcp\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "\n",
        "!apt-get install software-properties-common\n",
        "\n",
        "!apt-get install -y -qq software-properties-common module-init-tools\n",
        "\n",
        "!apt-get install -y -qq python-software-properties module-init-tools\n",
        "\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "software-properties-common is already the newest version (0.96.24.32.11).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.\n",
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 135004 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.17-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.17-0ubuntu1~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.17-0ubuntu1~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3go9n3JVzknk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Handling outputs\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def handle_pose(output, input_shape):\n",
        "    '''\n",
        "    Handles the output of the Pose Estimation model.\n",
        "    Returns ONLY the keypoint heatmaps, and not the Part Affinity Fields.\n",
        "    '''\n",
        "    # TODO 1: Extract only the second blob output (keypoint heatmaps)\n",
        "    heatmaps = output['Mconv7_stage2_L1']\n",
        "    # TODO 2: Resize the heatmap back to the size of the input\n",
        "    out_heatmap = np.zeros([heatmaps.shape[1], input_shape[0], input_shape[1]])\n",
        "    for h in range(len(heatmaps[0])):\n",
        "        out_heatmap[h] = cv2.resize(heatmaps[0][h],\n",
        "                                   input_shape[0:2][::-1])\n",
        "    return out_heatmap\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "def handle_text(output, input_shape):\n",
        "    '''\n",
        "    Handles the output of the Text Detection model.\n",
        "    Returns ONLY the text/no text classification of each pixel,\n",
        "        and not the linkage between pixels and their neighbors.\n",
        "    '''\n",
        "    # TODO 1: Extract only the first blob output (text/no text classification)\n",
        "    text_classes = output['model/segm_logits/add']\n",
        "    # TODO 2: Resize this output back to the size of the input\n",
        "    out_text = np.empty([text_classes.shape[1], input_shape[0], input_shape[1]])\n",
        "    for t in range (len(text_classes[0])):\n",
        "        out_text[t] = cv2.resize(text_classes[0][t], input_shape[0:2][::-1])\n",
        "    return out_text\n",
        "\n",
        "     \n",
        "\n",
        "\n",
        "def handle_car(output, input_shape):\n",
        "    '''\n",
        "    Handles the output of the Car Metadata model.\n",
        "    Returns two integers: the argmax of each softmax output.\n",
        "    The first is for color, and the second for type.\n",
        "    '''\n",
        "    # TODO 1: Get the argmax of the \"color\" output\n",
        "    color = output['color'].flatten()\n",
        "    car_type = output['type'].flatten()\n",
        "    color_class = np.argmax(color)\n",
        "    # TODO 2: Get the argmax of the \"type\" output\n",
        "    type_class = np.argmax(car_type)\n",
        "    \n",
        "    \n",
        "    return color_class, type_class\n",
        "\n",
        "\n",
        "def handle_output(model_type):\n",
        "    '''\n",
        "    Returns the related function to handle an output,\n",
        "        based on the model_type being used.\n",
        "    '''\n",
        "    if model_type == \"POSE\":\n",
        "        return handle_pose\n",
        "    elif model_type == \"TEXT\":\n",
        "        return handle_text\n",
        "    elif model_type == \"CAR_META\":\n",
        "        return handle_car\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "'''\n",
        "The below function is carried over from the previous exercise.\n",
        "You just need to call it appropriately in `app.py` to preprocess\n",
        "the input image.\n",
        "'''\n",
        "def preprocessing(input_image, height, width):\n",
        "    '''\n",
        "    Given an input image, height and width:\n",
        "    - Resize to width and height\n",
        "    - Transpose the final \"channel\" dimension to be first\n",
        "    - Reshape the image to add a \"batch\" of 1 at the start \n",
        "    '''\n",
        "    image = np.copy(input_image)\n",
        "    image = cv2.resize(image, (width, height))\n",
        "    image = image.transpose((2,0,1))\n",
        "    image = image.reshape(1, 3, height, width)\n",
        "\n",
        "    return image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZS8rs_a0LES",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "from handle_models import handle_output, preprocessing\n",
        "from inference import Network\n",
        "\n",
        "\n",
        "CAR_COLORS = [\"white\", \"gray\", \"yellow\", \"red\", \"green\", \"blue\", \"black\"]\n",
        "CAR_TYPES = [\"car\", \"bus\", \"truck\", \"van\"]\n",
        "\n",
        "\n",
        "def get_args():\n",
        "    '''\n",
        "    Gets the arguments from the command line.\n",
        "    '''\n",
        "\n",
        "    parser = argparse.ArgumentParser(\"Basic Edge App with Inference Engine\")\n",
        "    # -- Create the descriptions for the commands\n",
        "\n",
        "    c_desc = \"CPU extension file location, if applicable\"\n",
        "    d_desc = \"Device, if not CPU (GPU, FPGA, MYRIAD)\"\n",
        "    i_desc = \"The location of the input image\"\n",
        "    m_desc = \"The location of the model XML file\"\n",
        "    t_desc = \"The type of model: POSE, TEXT or CAR_META\"\n",
        "\n",
        "    # -- Add required and optional groups\n",
        "    parser._action_groups.pop()\n",
        "    required = parser.add_argument_group('required arguments')\n",
        "    optional = parser.add_argument_group('optional arguments')\n",
        "\n",
        "    # -- Create the arguments\n",
        "    required.add_argument(\"-i\", help=i_desc, required=True)\n",
        "    required.add_argument(\"-m\", help=m_desc, required=True)\n",
        "    required.add_argument(\"-t\", help=t_desc, required=True)\n",
        "    optional.add_argument(\"-c\", help=c_desc, default=None)\n",
        "    optional.add_argument(\"-d\", help=d_desc, default=\"CPU\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    return args\n",
        "\n",
        "\n",
        "def get_mask(processed_output):\n",
        "    '''\n",
        "    Given an input image size and processed output for a semantic mask,\n",
        "    returns a masks able to be combined with the original image.\n",
        "    '''\n",
        "    # Create an empty array for other color channels of mask\n",
        "    empty = np.zeros(processed_output.shape)\n",
        "    # Stack to make a Green mask where text detected\n",
        "    mask = np.dstack((empty, processed_output, empty))\n",
        "\n",
        "    return mask\n",
        "\n",
        "\n",
        "def create_output_image(model_type, image, output):\n",
        "    '''\n",
        "    Using the model type, input image, and processed output,\n",
        "    creates an output image showing the result of inference.\n",
        "    '''\n",
        "    if model_type == \"POSE\":\n",
        "        # Remove final part of output not used for heatmaps\n",
        "        output = output[:-1]\n",
        "        # Get only pose detections above 0.5 confidence, set to 255\n",
        "        for c in range(len(output)):\n",
        "            output[c] = np.where(output[c]>0.5, 255, 0)\n",
        "        # Sum along the \"class\" axis\n",
        "        output = np.sum(output, axis=0)\n",
        "        # Get semantic mask\n",
        "        pose_mask = get_mask(output)\n",
        "        # Combine with original image\n",
        "        image = image + pose_mask\n",
        "        return image\n",
        "    elif model_type == \"TEXT\":\n",
        "        # Get only text detections above 0.5 confidence, set to 255\n",
        "        output = np.where(output[1]>0.5, 255, 0)\n",
        "        # Get semantic mask\n",
        "        text_mask = get_mask(output)\n",
        "        # Add the mask to the image\n",
        "        image = image + text_mask\n",
        "        return image\n",
        "    elif model_type == \"CAR_META\":\n",
        "        # Get the color and car type from their lists\n",
        "        color = CAR_COLORS[output[0]]\n",
        "        car_type = CAR_TYPES[output[1]]\n",
        "        # Scale the output text by the image shape\n",
        "        scaler = max(int(image.shape[0] / 1000), 1)\n",
        "        # Write the text of color and type onto the image\n",
        "        image = cv2.putText(image, \n",
        "            \"Color: {}, Type: {}\".format(color, car_type), \n",
        "            (50 * scaler, 100 * scaler), cv2.FONT_HERSHEY_SIMPLEX, \n",
        "            2 * scaler, (255, 255, 255), 3 * scaler)\n",
        "        return image\n",
        "    else:\n",
        "        print(\"Unknown model type, unable to create output image.\")\n",
        "        return image\n",
        "\n",
        "\n",
        "def perform_inference(args):\n",
        "    '''\n",
        "    Performs inference on an input image, given a model.\n",
        "    '''\n",
        "    # Create a Network for using the Inference Engine\n",
        "    inference_network = Network()\n",
        "    # Load the model in the network, and obtain its input shape\n",
        "    n, c, h, w = inference_network.load_model(args.m, args.d, args.c)\n",
        "\n",
        "    # Read the input image\n",
        "    image = cv2.imread(args.i)\n",
        "\n",
        "    ### TODO: Preprocess the input image\n",
        "    preprocessed_image = preprocessing(image, h, w)\n",
        "\n",
        "    # Perform synchronous inference on the image\n",
        "    inference_network.sync_inference(preprocessed_image)\n",
        "\n",
        "    # Obtain the output of the inference request\n",
        "    output = inference_network.extract_output()\n",
        "\n",
        "    ### TODO: Handle the output of the network, based on args.t\n",
        "    ### Note: This will require using `handle_output` to get the correct\n",
        "    ###       function, and then feeding the output to that function.\n",
        "    process_func = handle_output(args.t)\n",
        "    processed_output = process_func(output, image.shape)\n",
        "\n",
        "\n",
        "    # Create an output image based on network\n",
        "    try:\n",
        "        output_image = create_output_image(args.t, image, processed_output)\n",
        "        print(\"success\")\n",
        "    except:\n",
        "        output_image = image\n",
        "        print(\"Error\")\n",
        "\n",
        "    # Save down the resulting image\n",
        "    cv2.imwrite(\"outputs/{}-output.png\".format(args.t), output_image)\n",
        "\n",
        "\n",
        "def main():\n",
        "    args = get_args()\n",
        "    perform_inference(args)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}